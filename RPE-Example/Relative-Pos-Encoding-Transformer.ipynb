{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f625897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daeabf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e161596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention1(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, length):\n",
    "        super().__init__()\n",
    "        self.n_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.length = length\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "    def split_proj(self, inputs, batch_size):\n",
    "        shape = (batch_size, -1, self.n_heads, self.d_head)\n",
    "        splitted_inputs = tf.reshape(inputs, shape=shape)\n",
    "        return tf.transpose(splitted_inputs, perm=[0,2,1,3])\n",
    "    \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        attention = tf.transpose(attention, perm=[0,2,1,3])\n",
    "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dada71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = layers.Dense(d_model, activation='relu')\n",
    "        self.wk = layers.Dense(d_model, activation='relu')\n",
    "        self.wv = layers.Dense(d_model, activation='relu')\n",
    "        self.dense = layers.Dense(d_model, activation='relu')\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def _generate_relative_position_matrix(self, length, max_relative_position, cache=False):\n",
    "        if not cache:\n",
    "            range_vec = tf.range(length)\n",
    "            range_mat = tf.reshape(tf.tile(range_vec, [length]), [length, length])\n",
    "            distance_mat = range_mat - tf.transpose(range_mat)\n",
    "        else:\n",
    "            distance_mat = tf.expand_dims(tf.range(-length+1, 1, 1), 0)\n",
    "        distance_mat_clipped = tf.clip_by_value(distance_mat, -max_relative_position, max_relative_position)\n",
    "        final_mat = distance_mat_clipped + max_relative_position\n",
    "    \n",
    "    def _generate_relative_positions_embeddings(self, length, depth, max_relative_position, name, cache=False):\n",
    "        with tf.compat.v1.variable_scope(name):\n",
    "            relative_positions_matrix = self._generate_relative_positions_matrix(length, max_relative_position, cache=cache)\n",
    "            vocab_size = max_relative_position * 2 + 1\n",
    "            embeddings_table = tf.compat.v1.get_variable(name, [vocab_size, depth])\n",
    "            embeddings = tf.gather(embeddings_table, relative_positions_matrix)\n",
    "            return embeddings\n",
    "    \n",
    "    def _relative_attention_inner(self, x, y, z, transpose, mask, length):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        heads = N_HEADS\n",
    "        xy_matmul = tf.matmul(x, y, transpose_b=transpose)\n",
    "        x_t = tf.transpose(x, [2, 0, 1, 3])\n",
    "        x_t_r = tf.reshape(x_t, [length, heads * batch_size, -1])\n",
    "        x_tz_matmul = tf.matmul(x_t_r, z, transpose_b=transpose)\n",
    "        x_tz_matmul_r = tf.reshape(x_tz_matmul, [length, batch_size, heads, -1])\n",
    "        x_tz_matmul_r_t = tf.transpose(x_tz_matmul_r, [1,2,0,3])\n",
    "        return tf.math.add(xy_matmul, x_tz_matmul_r_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e073af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [-1,  0,  1,  2,  3],\n",
       "       [-2, -1,  0,  1,  2],\n",
       "       [-3, -2, -1,  0,  1],\n",
       "       [-4, -3, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.tile(tf.range(5), [5]), [5, 5]) - tf.transpose(tf.reshape(tf.tile(tf.range(5), [5]), [5, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff6c6776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[-4, -3, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(tf.range(-5+1, 1, 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd4b231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  2,  2],\n",
       "       [-1,  0,  1,  2,  2],\n",
       "       [-2, -1,  0,  1,  2],\n",
       "       [-2, -2, -1,  0,  1],\n",
       "       [-2, -2, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.clip_by_value(tf.reshape(tf.tile(tf.range(5), [5]), [5, 5]) - tf.transpose(tf.reshape(tf.tile(tf.range(5), [5]), [5, 5])), -2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1941b6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=int32, numpy=\n",
       "array([[2, 3, 4, 4, 4],\n",
       "       [1, 2, 3, 4, 4],\n",
       "       [0, 1, 2, 3, 4],\n",
       "       [0, 0, 1, 2, 3],\n",
       "       [0, 0, 0, 1, 2]], dtype=int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.clip_by_value(tf.reshape(tf.tile(tf.range(5), [5]), [5, 5]) - tf.transpose(tf.reshape(tf.tile(tf.range(5), [5]), [5, 5])), -2, 2)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3467118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "      def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model  # typically 512\n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "        self.wk = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "        self.wv = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model, activation='relu')\n",
    "       \n",
    "      def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "     \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "      def _generate_relative_positions_matrix(self, length, max_relative_position,\n",
    "                                            cache=False):\n",
    "        \"\"\"Generates matrix of relative positions between inputs.\"\"\"\n",
    "        if not cache:\n",
    "          range_vec = tf.range(length)\n",
    "          range_mat = tf.reshape(tf.tile(range_vec, [length]), [length, length])\n",
    "          distance_mat = range_mat - tf.transpose(range_mat)\n",
    "        else:\n",
    "          distance_mat = tf.expand_dims(tf.range(-length+1, 1, 1), 0)\n",
    "        distance_mat_clipped = tf.clip_by_value(distance_mat, -max_relative_position,\n",
    "                                                max_relative_position)\n",
    "        # Shift values to be >= 0. Each integer still uniquely identifies a relative\n",
    "        # position difference.\n",
    "     \n",
    "        final_mat = distance_mat_clipped + max_relative_position\n",
    "        \n",
    "     \n",
    "        return final_mat\n",
    "\n",
    "      def _generate_relative_positions_embeddings(self, length, depth,\n",
    "                                                  max_relative_position, name,\n",
    "                                                  cache=False):\n",
    "        \"\"\"Generates tensor of size [1 if cache else length, length, depth].\"\"\"\n",
    "        with tf.compat.v1.variable_scope(name):\n",
    "          \n",
    "          relative_positions_matrix = self._generate_relative_positions_matrix(\n",
    "              length, max_relative_position, cache=cache)\n",
    "          vocab_size = max_relative_position * 2 + 1\n",
    "          # Generates embedding for each relative position of dimension depth.\n",
    "          embeddings_table = tf.compat.v1.get_variable(name, [vocab_size, depth])\n",
    "          embeddings = tf.gather(embeddings_table, relative_positions_matrix)\n",
    "       \n",
    "          return embeddings \n",
    "\n",
    "      def _relative_attention_inner(self, x, y, z, transpose,mask,length):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        heads = N_HEADS\n",
    "        \n",
    "                                                                   \n",
    "        # xy_matmul is [batch_size, heads, length or 1, length or depth]\n",
    "        xy_matmul = tf.matmul(x, y, transpose_b=transpose)\n",
    "       \n",
    "        # x_t is [length or 1, batch_size, heads, length or depth]\n",
    "        x_t = tf.transpose(x, [2, 0, 1, 3])\n",
    "        \n",
    "        # x_t_r is [length or 1, batch_size * heads, length or depth]\n",
    "        x_t_r = tf.reshape(x_t, [length, heads * batch_size, -1])\n",
    "        \n",
    "        # x_tz_matmul is [length or 1, batch_size * heads, length or depth]\n",
    "        x_tz_matmul = tf.matmul(x_t_r, z, transpose_b=transpose) \n",
    "       \n",
    "        # x_tz_matmul_r is [length or 1, batch_size, heads, length or depth]\n",
    "        x_tz_matmul_r = tf.reshape(x_tz_matmul, [length, batch_size, heads, -1])\n",
    "        \n",
    "        # x_tz_matmul_r_t is [batch_size, heads, length or 1, length or depth]\n",
    "        x_tz_matmul_r_t = tf.transpose(x_tz_matmul_r, [1, 2, 0, 3])\n",
    "     \n",
    "      \n",
    "        \n",
    "        return tf.math.add(xy_matmul, x_tz_matmul_r_t)\n",
    "\n",
    "      def dot_product_attention_relative(self,length, q,\n",
    "                                      k,\n",
    "                                      v,\n",
    "                                      mask,\n",
    "                                      bias,\n",
    "                                      max_relative_position,\n",
    "                                      dropout_rate=0.0,\n",
    "                                      image_shapes=None,\n",
    "                                      save_weights_to=None,\n",
    "                                      name=None,\n",
    "                                      make_image_summary=True,\n",
    "                                      cache=False):\n",
    "        if not max_relative_position:\n",
    "          raise ValueError(\"Max relative position (%s) should be > 0 when using \"\n",
    "                          \"relative self attention.\" % (max_relative_position))\n",
    "        with tf.compat.v1.variable_scope(\n",
    "            name, default_name=\"dot_product_attention_relative\",\n",
    "            values=[q, k, v]) as scope:\n",
    "\n",
    "          # This calculation only works for self attention.\n",
    "          # q, k and v must therefore have the same shape.\n",
    "          if not cache:\n",
    "            q.get_shape().assert_is_compatible_with(k.get_shape())\n",
    "            q.get_shape().assert_is_compatible_with(v.get_shape())\n",
    "          \n",
    "          # Use separate embeddings suitable for keys and values.\n",
    "          depth = self.depth #64 #128   #self.depth 512/4\n",
    "         \n",
    "          # print('Error...')\n",
    "          relations_keys = self._generate_relative_positions_embeddings(length, depth, max_relative_position, \"relative_positions_keys\",cache=cache)\n",
    "          \n",
    "          relations_values = self._generate_relative_positions_embeddings(length, depth, max_relative_position, \"relative_positions_values\",cache=cache)\n",
    "          \n",
    "          # Compute self attention considering the relative position embeddings.\n",
    "        \n",
    "          logits = self._relative_attention_inner(q, k, relations_keys, True,mask,length)\n",
    "          #print(\"logits\",logits.shape,mask.shape)\n",
    "          logits += (mask * -1e9)\n",
    "          weights = tf.nn.softmax(logits)\n",
    "          return self._relative_attention_inner(weights, v, relations_values, False,None,length), weights\n",
    "    \n",
    "      def call(self,length, q, k, v, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "       \n",
    "        oot, attweight = self.dot_product_attention_relative(length,q, k, v, mask,bias=None, max_relative_position=16, dropout_rate=0.1,image_shapes=None,save_weights_to=None,name=None,make_image_summary=False,cache=False)\n",
    "        scaled_attention = tf.transpose(oot, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        output = tf.reshape(output, (batch_size, length, self.d_model))\n",
    "        \n",
    "        return output#, attweight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_relative_positions_matrix(self, length, max_relative_position,\n",
    "                                            cache=False):\n",
    "        \"\"\"Generates matrix of relative positions between inputs.\"\"\"\n",
    "        if not cache:\n",
    "          range_vec = torch.arange(length) \n",
    "          range_mat = torch.tile(range_vec, [length]).view(length, length)\n",
    "          distance_mat = range_mat - torch.transpose(range_mat)\n",
    "        else:\n",
    "          distance_mat = torch.arange(-length+1, 1, 1)[None, :]\n",
    "        distance_mat_clipped = torch.clamp(distance_mat, -max_relative_position,\n",
    "                                      max_relative_position)\n",
    "        # Shift values to be >= 0. Each integer still uniquely identifies a relative\n",
    "        # position difference.\n",
    "     \n",
    "        final_mat = distance_mat_clipped + max_relative_position\n",
    "        \n",
    "     \n",
    "        return final_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_relative_positions_embeddings(self, length, depth,\n",
    "                                                  max_relative_position, name,\n",
    "                                                  cache=False):\n",
    "        \"\"\"Generates tensor of size [1 if cache else length, length, depth].\"\"\"\n",
    "          relative_positions_matrix = self._generate_relative_positions_matrix(\n",
    "              length, max_relative_position, cache=cache)\n",
    "          vocab_size = max_relative_position * 2 + 1\n",
    "          # Generates embedding for each relative position of dimension depth.\n",
    "          embeddings_table = tf.compat.v1.get_variable(name, [vocab_size, depth])\n",
    "          embeddings = tf.gather(embeddings_table, relative_positions_matrix)\n",
    "       \n",
    "          return embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f6ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawNumpyFeatureLoader(DataLoader):\n",
    "    \"\"\"\n",
    "    Use DataLoader to make texts into batch\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                batch_size,\n",
    "                root_dir,\n",
    "                ann_path, \n",
    "                feat_dir, \n",
    "                text_dir, \n",
    "                **kwargs):\n",
    "       \n",
    "        self.dataset = NumpyFeatureDataset(\n",
    "            root_dir, ann_path,  feat_dir, text_dir)\n",
    "\n",
    "        self.collate_fn = self.dataset.collate_fn\n",
    "        \n",
    "        super(RawNumpyFeatureLoader, self).__init__(\n",
    "            self.dataset,\n",
    "            batch_size=batch_size,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self.collate_fn,\n",
    "            **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyFeatureLoader(DataLoader):\n",
    "    \"\"\"\n",
    "    Use BucketIterator to make texts of same length into batch\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                batch_size,\n",
    "                root_dir,\n",
    "                ann_path, \n",
    "                feat_dir, \n",
    "                text_dir, \n",
    "                device,\n",
    "                **kwargs):\n",
    "       \n",
    "        self.dataset = abc\n",
    "\n",
    "        self.collate_fn = self.dataset.collate_fn\n",
    "        \n",
    "        super(NumpyFeatureLoader, self).__init__(\n",
    "            self.dataset,\n",
    "            batch_size=batch_size,\n",
    "            device=device, \n",
    "            sort_key=lambda x: len(x['text']),\n",
    "            repeat=True, # Repeat the iterator for multiple epochs.\n",
    "            sort=False,  # Sort all examples in data using `sort_key`.\n",
    "            shuffle=True, # Shuffle data on each epoch run.\n",
    "            sort_within_batch=True,\n",
    "            **kwargs) # Use `sort_key` to sort examples in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "852f3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a24417af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataLoader in module torch.utils.data.dataloader:\n",
      "\n",
      "class DataLoader(typing.Generic)\n",
      " |  DataLoader(*args, **kwds)\n",
      " |  \n",
      " |  Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
      " |  the given dataset.\n",
      " |  \n",
      " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
      " |  iterable-style datasets with single- or multi-process loading, customizing\n",
      " |  loading order and optional automatic batching (collation) and memory pinning.\n",
      " |  \n",
      " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
      " |  \n",
      " |  Args:\n",
      " |      dataset (Dataset): dataset from which to load the data.\n",
      " |      batch_size (int, optional): how many samples per batch to load\n",
      " |          (default: ``1``).\n",
      " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      " |          at every epoch (default: ``False``).\n",
      " |      sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
      " |          samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
      " |          implemented. If specified, :attr:`shuffle` must not be specified.\n",
      " |      batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
      " |          returns a batch of indices at a time. Mutually exclusive with\n",
      " |          :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
      " |          and :attr:`drop_last`.\n",
      " |      num_workers (int, optional): how many subprocesses to use for data\n",
      " |          loading. ``0`` means that the data will be loaded in the main process.\n",
      " |          (default: ``0``)\n",
      " |      collate_fn (callable, optional): merges a list of samples to form a\n",
      " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
      " |          map-style dataset.\n",
      " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
      " |          into CUDA pinned memory before returning them.  If your data elements\n",
      " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
      " |          see the example below.\n",
      " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
      " |          the size of dataset is not divisible by the batch size, then the last batch\n",
      " |          will be smaller. (default: ``False``)\n",
      " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      " |          from workers. Should always be non-negative. (default: ``0``)\n",
      " |      worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
      " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      " |          input, after seeding and before data loading. (default: ``None``)\n",
      " |      generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
      " |          by RandomSampler to generate random indexes and multiprocessing to generate\n",
      " |          `base_seed` for workers. (default: ``None``)\n",
      " |      prefetch_factor (int, optional, keyword-only arg): Number of samples loaded\n",
      " |          in advance by each worker. ``2`` means there will be a total of\n",
      " |          2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
      " |      persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
      " |          the worker processes after a dataset has been consumed once. This allows to\n",
      " |          maintain the workers `Dataset` instances alive. (default: ``False``)\n",
      " |  \n",
      " |  \n",
      " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
      " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
      " |               :ref:`multiprocessing-best-practices` on more details related\n",
      " |               to multiprocessing in PyTorch.\n",
      " |  \n",
      " |  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
      " |               When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
      " |               it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
      " |               rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
      " |               configurations. This represents the best guess PyTorch can make because PyTorch\n",
      " |               trusts user :attr:`dataset` code in correctly handling multi-process\n",
      " |               loading to avoid duplicate data.\n",
      " |  \n",
      " |               However, if sharding results in multiple workers having incomplete last batches,\n",
      " |               this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
      " |               be broken into multiple ones and (2) more than one batch worth of samples can be\n",
      " |               dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
      " |               cases in general.\n",
      " |  \n",
      " |               See `Dataset Types`_ for more details on these two types of datasets and how\n",
      " |               :class:`~torch.utils.data.IterableDataset` interacts with\n",
      " |               `Multi-process data loading`_.\n",
      " |  \n",
      " |  .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
      " |               :ref:`data-loading-randomness` notes for random seed related questions.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataLoader\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, dataset: torch.utils.data.dataset.Dataset[+T_co], batch_size: Union[int, NoneType] = 1, shuffle: bool = False, sampler: Union[torch.utils.data.sampler.Sampler, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[Sequence], NoneType] = None, num_workers: int = 0, collate_fn: Union[Callable[[List[~T]], Any], NoneType] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Union[Callable[[int], NoneType], NoneType] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self) -> '_BaseDataLoaderIter'\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  __setattr__(self, attr, val)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  check_worker_number_rationality(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  multiprocessing_context\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_iterator': typing.Union[ForwardRef('_BaseDataLoad...\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  __parameters__ = (+T_co,)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "468b9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee73888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePosition(nn.Module):\n",
    "\n",
    "    def __init__(self, num_units, max_relative_position):\n",
    "        super().__init__()\n",
    "        self.num_units = num_units\n",
    "        self.max_relative_position = max_relative_position\n",
    "        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n",
    "        nn.init.xavier_uniform_(self.embeddings_table)\n",
    "\n",
    "    def forward(self, length_q, length_k):\n",
    "        range_vec_q = torch.arange(length_q)\n",
    "        range_vec_k = torch.arange(length_k)\n",
    "        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n",
    "        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n",
    "        final_mat = distance_mat_clipped + self.max_relative_position\n",
    "        final_mat = torch.LongTensor(final_mat)\n",
    "        print(final_mat)\n",
    "        print(self.embeddings_table)\n",
    "        embeddings = self.embeddings_table[final_mat]\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13dd8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative = RelativePosition(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35d6fe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9, 10, 10, 10],\n",
      "        [ 4,  5,  6,  7,  8,  9, 10, 10],\n",
      "        [ 3,  4,  5,  6,  7,  8,  9, 10],\n",
      "        [ 2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [ 0,  0,  1,  2,  3,  4,  5,  6]])\n",
      "Parameter containing:\n",
      "tensor([[-0.1902, -0.5252,  0.5227,  0.5277,  0.1122,  0.0997,  0.3418,  0.3685,\n",
      "         -0.0089, -0.0805],\n",
      "        [ 0.0106, -0.3891,  0.2292, -0.3254, -0.2666, -0.1337,  0.2731,  0.3179,\n",
      "         -0.4902,  0.1341],\n",
      "        [-0.3047,  0.3478,  0.5268, -0.0111,  0.4231, -0.3569, -0.4623, -0.3676,\n",
      "          0.0249,  0.0734],\n",
      "        [-0.1396, -0.3121, -0.1920, -0.2770,  0.5302,  0.4391,  0.0854,  0.0637,\n",
      "         -0.1859,  0.3874],\n",
      "        [ 0.1061, -0.2006,  0.3348,  0.0188,  0.4034,  0.1066, -0.4279, -0.3755,\n",
      "          0.1734,  0.1670],\n",
      "        [-0.5014, -0.4008,  0.1953,  0.3884,  0.1437, -0.4882,  0.3691,  0.1262,\n",
      "         -0.0872,  0.3135],\n",
      "        [ 0.4184, -0.2667, -0.3069, -0.3300,  0.2172, -0.0719, -0.2473,  0.3329,\n",
      "         -0.2623,  0.1828],\n",
      "        [-0.0414, -0.1599, -0.5302,  0.4881,  0.1302,  0.4051, -0.1331,  0.1400,\n",
      "         -0.1122,  0.3326],\n",
      "        [ 0.4151,  0.4150,  0.0733, -0.2498, -0.0488,  0.4171, -0.3554,  0.2386,\n",
      "          0.1389, -0.0041],\n",
      "        [ 0.2529, -0.2041,  0.2825,  0.3817, -0.2926, -0.4807, -0.2242,  0.2380,\n",
      "          0.2747, -0.0871],\n",
      "        [ 0.2110,  0.0599, -0.3010,  0.2514,  0.1023,  0.0371,  0.1733,  0.0516,\n",
      "         -0.1412, -0.2282]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = relative(7, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "914fb5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 8, 10])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea4c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
