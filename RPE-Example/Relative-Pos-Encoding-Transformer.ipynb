{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f625897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daeabf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e161596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention1(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, length):\n",
    "        super().__init__()\n",
    "        self.n_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.length = length\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "    def split_proj(self, inputs, batch_size):\n",
    "        shape = (batch_size, -1, self.n_heads, self.d_head)\n",
    "        splitted_inputs = tf.reshape(inputs, shape=shape)\n",
    "        return tf.transpose(splitted_inputs, perm=[0,2,1,3])\n",
    "    \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        attention = tf.transpose(attention, perm=[0,2,1,3])\n",
    "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dada71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = layers.Dense(d_model, activation='relu')\n",
    "        self.wk = layers.Dense(d_model, activation='relu')\n",
    "        self.wv = layers.Dense(d_model, activation='relu')\n",
    "        self.dense = layers.Dense(d_model, activation='relu')\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def _generate_relative_position_matrix(self, length, max_relative_position, cache=False):\n",
    "        if not cache:\n",
    "            range_vec = tf.range(length)\n",
    "            range_mat = tf.reshape(tf.tile(range_vec, [length]), [length, length])\n",
    "            distance_mat = range_mat - tf.transpose(range_mat)\n",
    "        else:\n",
    "            distance_mat = tf.expand_dims(tf.range(-length+1, 1, 1), 0)\n",
    "        distance_mat_clipped = tf.clip_by_value(distance_mat, -max_relative_position, max_relative_position)\n",
    "        final_mat = distance_mat_clipped + max_relative_position\n",
    "    \n",
    "    def _generate_relative_positions_embeddings(self, length, depth, max_relative_position, name, cache=False):\n",
    "        with tf.compat.v1.variable_scope(name):\n",
    "            relative_positions_matrix = self._generate_relative_positions_matrix(length, max_relative_position, cache=cache)\n",
    "            vocab_size = max_relative_position * 2 + 1\n",
    "            embeddings_table = tf.compat.v1.get_variable(name, [vocab_size, depth])\n",
    "            embeddings = tf.gather(embeddings_table, relative_positions_matrix)\n",
    "            return embeddings\n",
    "    \n",
    "    def _relative_attention_inner(self, x, y, z, transpose, mask, length):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        heads = N_HEADS\n",
    "        xy_matmul = tf.matmul(x, y, transpose_b=transpose)\n",
    "        x_t = tf.transpose(x, [2, 0, 1, 3])\n",
    "        x_t_r = tf.reshape(x_t, [length, heads * batch_size, -1])\n",
    "        x_tz_matmul = tf.matmul(x_t_r, z, transpose_b=transpose)\n",
    "        x_tz_matmul_r = tf.reshape(x_tz_matmul, [length, batch_size, heads, -1])\n",
    "        x_tz_matmul_r_t = tf.transpose(x_tz_matmul_r, [1,2,0,3])\n",
    "        return tf.math.add(xy_matmul, x_tz_matmul_r_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e073af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [-1,  0,  1,  2,  3],\n",
       "       [-2, -1,  0,  1,  2],\n",
       "       [-3, -2, -1,  0,  1],\n",
       "       [-4, -3, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.tile(tf.range(5), [5]), [5, 5]) - tf.transpose(tf.reshape(tf.tile(tf.range(5), [5]), [5, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff6c6776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[-4, -3, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(tf.range(-5+1, 1, 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd4b231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  2,  2],\n",
       "       [-1,  0,  1,  2,  2],\n",
       "       [-2, -1,  0,  1,  2],\n",
       "       [-2, -2, -1,  0,  1],\n",
       "       [-2, -2, -2, -1,  0]], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.clip_by_value(tf.reshape(tf.tile(tf.range(5), [5]), [5, 5]) - tf.transpose(tf.reshape(tf.tile(tf.range(5), [5]), [5, 5])), -2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1941b6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=int32, numpy=\n",
       "array([[2, 3, 4, 4, 4],\n",
       "       [1, 2, 3, 4, 4],\n",
       "       [0, 1, 2, 3, 4],\n",
       "       [0, 0, 1, 2, 3],\n",
       "       [0, 0, 0, 1, 2]], dtype=int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.clip_by_value(tf.reshape(tf.tile(tf.range(5), [5]), [5, 5]) - tf.transpose(tf.reshape(tf.tile(tf.range(5), [5]), [5, 5])), -2, 2)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176fa84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
