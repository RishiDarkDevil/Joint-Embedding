{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d747eb05",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e98fe",
   "metadata": {},
   "source": [
    "Importing all the necessary libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ca4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "# from torchviz import make_dot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "from daam import trace, set_seed, plot_overlay_heat_map, expand_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd8dcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/rishideychowdhury/.huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20bd6def",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') # Set it to 'cuda' for gpu or 'cpu' for cpu or 'mps' for M1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39051edf",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbad763",
   "metadata": {},
   "source": [
    "I am using the **Flickr30k Dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f854f6",
   "metadata": {},
   "source": [
    "I define the class that will handle the custom dataset. The format in which the dataset of images and the annotation files are present is according to the COCO guidelines.\n",
    "\n",
    "Make sure to have extracted the image and annotation features for the following class to function properly. The data directory tree is being provided below for reference. Suppose the folder containing all the data is named `data`.\n",
    "```\n",
    "data ───────────────────────────> Contains the Dataset and the relevant pre-computed features\n",
    "├── flickr30k_annotations ──────> Contains the Annotation Files for the Flickr30k Dataset\n",
    "│   ├── flickr30k_all.json ─────> Annotation File for the entire Flickr30k Images\n",
    "│   ├── train.json ─────────────> Annotation File for the train Flickr30k Images \n",
    "│   └── val.json ───────────────> Annotation File for the validation Flickr30k Images \n",
    "└── flickr30k_images ───────────> Contains the Image Files for the Flickr30k Dataset\n",
    "    └── flickr30k_images ───────> Ignore this directory\n",
    "        ├── 1000092795.jpg\n",
    "        ├── 10002456.jpg ───────> Images from the Flickr30k Dataset\n",
    "        ├── ...\n",
    "        └── 998845445.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9935edc",
   "metadata": {},
   "source": [
    "### Custom Dataset Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4133f",
   "metadata": {},
   "source": [
    "Following are the custom Dataset classes to handle our data and will serve has an input to our DataLoader for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf064e",
   "metadata": {},
   "source": [
    "#### Annotation COCO Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb1905",
   "metadata": {},
   "source": [
    "The following is the class that handles the dataset in COCO format. It deals with the raw captions only. The `AnnCocoDataset` class can be used to fetch the data in the form of:\n",
    "- `ann_ids`: IDs for the annotations in the .json annotation files\n",
    "- `tgt_texts_raw`: Raw caption texts for each image (Since, there are multiple captions for each image, we select a random caption out of the many for each image)\n",
    "- `texts`: Should contain the transformed preprocessed caption text, but as of now it returns `tgt_texts_raw`. **TODO!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1d2418f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnCocoDataset(Dataset):\n",
    "  \n",
    "  def __init__(self, ann_path):\n",
    "\n",
    "    self.ann_path = ann_path\n",
    "    \n",
    "    self.coco = COCO(ann_path)\n",
    "    self.image_ids = self.coco.getImgIds()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_ids)\n",
    "\n",
    "  def load_annotations(self, image_index, return_all=False):\n",
    "    ann_id = self.coco.getAnnIds(imgIds=self.image_ids[image_index])\n",
    "\n",
    "    if not return_all:\n",
    "      ann_id = ann_id[0] # A random annotation out of the many annotations is returned\n",
    "      anns = self.coco.loadAnns(ann_id)[0]['caption']\n",
    "    else:\n",
    "      anns = self.coco.loadAnns(ann_id)\n",
    "      anns = [i['caption'] for i in anns]\n",
    "    return anns, ann_id\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    text, ann_id = self.load_annotations(index)\n",
    "    return {\n",
    "      'ann_id': ann_id,\n",
    "      'text': text,\n",
    "  }\n",
    "\n",
    "  def collate_fn(self, batch):\n",
    "    ann_ids = [s['ann_id'] for s in batch]\n",
    "    texts = [s['text'] for s in batch]\n",
    "\n",
    "    return {\n",
    "      'ann_ids': ann_ids,\n",
    "      'tgt_texts_raw': texts,\n",
    "      'texts': texts,\n",
    "    }\n",
    "\n",
    "  def __str__(self): \n",
    "    s1 = \"Number of images: \" + str(len(self.image_ids)) + '\\n'\n",
    "    s2 = \"Number of texts: \" + str(len(self.coco.getAnnIds())) + '\\n'\n",
    "    return s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dc86297f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset = AnnCocoDataset(\n",
    "    ann_path=\"/Users/rishideychowdhury/Desktop/Joint-Embedding/Data/flickr30k_annotations/val.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026de74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cda3d71d",
   "metadata": {},
   "source": [
    "## Setting the Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d14ad7",
   "metadata": {},
   "source": [
    "I will be using `CompVis/stable-diffusion-v1-4` model(=`model_id`) to generate images corresponding to the captions provided in the `dataset` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e0a6581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"CompVis/stable-diffusion-v1-4\" # model id\n",
    "dev = 'mps' # set to 'cpu', 'cuda' or 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6990c22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3007abe92894fd89036004a0634eefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e0b5d6750e4a7fa335259cd196263f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ad10223e77451e8be02855f87a24f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a08171c8a524badbbf75cdeb3c57a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/71.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "  model_id, \n",
    "  use_auth_token=True\n",
    ")\n",
    "pipe = pipe.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2c4f228d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696da825bba5477c9cb91160492f1a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishideychowdhury/Desktop/Joint-Embedding/envdaam/lib/python3.10/site-packages/torch/amp/autocast_mode.py:202: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/Users/rishideychowdhury/Desktop/Joint-Embedding/envdaam/lib/python3.10/site-packages/torch/nn/functional.py:3958: UserWarning: The operator 'aten::upsample_bicubic2d.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  return torch._C._nn.upsample_bicubic2d(input, output_size, align_corners, scale_factors)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [123], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace(pipe) \u001b[38;5;28;01mas\u001b[39;00m tc:\n\u001b[1;32m      6\u001b[0m     out \u001b[38;5;241m=\u001b[39m pipe(prompt, num_inference_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     heat_map \u001b[38;5;241m=\u001b[39m \u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_global_heat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     heat_map \u001b[38;5;241m=\u001b[39m expand_image(heat_map\u001b[38;5;241m.\u001b[39mcompute_word_heat_map(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m     plot_overlay_heat_map(out\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m], heat_map)\n",
      "File \u001b[0;32m~/Desktop/Joint-Embedding/envdaam/lib/python3.10/site-packages/daam/trace.py:147\u001b[0m, in \u001b[0;36mDiffusionHeatMapHooker.compute_global_heat_map\u001b[0;34m(self, prompt, time_weights, time_idx, last_n, factors)\u001b[0m\n\u001b[1;32m    144\u001b[0m     all_merges\u001b[38;5;241m.\u001b[39mappend(merge_list)\n\u001b[1;32m    146\u001b[0m maps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mstack(x, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_merges], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m maps \u001b[38;5;241m=\u001b[39m \u001b[43mmaps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HeatMap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe\u001b[38;5;241m.\u001b[39mtokenizer, prompt, maps)\n",
      "File \u001b[0;32m~/Desktop/Joint-Embedding/envdaam/lib/python3.10/site-packages/torch/cuda/__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "prompt = 'A dog runs across the field'\n",
    "# gen = set_seed(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with trace(pipe) as tc:\n",
    "        out = pipe(prompt, num_inference_steps=30)\n",
    "        heat_map = tc.compute_global_heat_map(prompt)\n",
    "        heat_map = expand_image(heat_map.compute_word_heat_map('dog'))\n",
    "        plot_overlay_heat_map(out.images[0], heat_map)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb4819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
